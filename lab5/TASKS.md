# Пятая лабораторная работа.

### Тема: Применение нейронных сетей для прогнозирования числовых последовательностей.
### Цель: Ознакомиться, проанализировать и получить навыки реализации модели рекуррентной сети для задачи прогнозирования.

## Задание по четвёртой лабораторной работе по моделям решения задач в интеллектуальных системах.

### Дано:
1. текст методических указаний по лабораторной работе;
2. учебные материалы по программированию, основам нейросетевых моделей и другие;
3. номер варианта задачи.

### Требуется:
1. построить модель рекуррентной сети, реализующей решение задачи прогнозирования, удовлетворяющую нижеприведённым требованиям;
2. построить семейства графиков зависимостей параметров модели;
3. дать ответы на вопросы из списка;
4. предоставить письменный отчёт.

## Требования к модели рекуррентной сети.
*   Опциональное зануление значений контекстных нейронов на каждой эпохе.
*   Опциональное указание линейной функции активации эффекторов.
*   Для сетей с долгой кратковременной памятью и управляемыми рекуррентными блоками использовать линейную или недовыпрямленную линейную функцию активации эффекторных нейронов.
*   Пользовательская настройка параметра экспоненциальной и недовыпрямленной линейных функций активации.
*   Пользовательская настройка параметров сети, включая размер скользящего окна, количество контекстных нейронов и нейронов на скрытом слое.
*   Пользовательская настройка количества эффекторных нейронов для сетей Джордана и Джордана-Элмана.
*   Заданные для прогнозирования последовательности должны иметь не менее десяти эталонных значений.
*   Отображение прогнозируемых значений на область выходных значений эффекторов нейронной сети.
*   Последовательный расчёт нескольких (не менее трех) прогнозных значений методом скользящего окна с сохранением значений контекстных нейронов.
*   Прогнозирование значений апериодических и периодических последовательностей, последовательности чисел Фибоначчи, арифметических и геометрических прогрессий, значений степенных функций, факториала и других:
    *   1, 0.5, 0.25, 0.125, 0.625...
    *   1, 0.5, 0.(3), 0.25, 0.2...
    *   1, 0, -1, 0, 1, 0, -1, 0...
    *   1, 0, 1, 0, 0, 1, 0, 0, 0, 1...
    *   1, 2, 3, 4, 5, 6, 7...
    *   1, 4, 9, 16, 25...
    *   1, 2, 3, 5, 8...
    *   1, 2, 4, 8, 16...
    *   1, 2, 5, 15, 52...
    *   1, 2, 6, 24, 120...

## Требования и порядок выполнения лабораторной работы:
1.  Продумать вариант решения задачи.
2.  Выбрать и описать структуры данных для хранения получаемых и обрабатываемых данных.
3.  Указать авторство во всех файлах в заданном формате указания авторства.
4.  Запрограммировать процедуры ввода и обработки данных, представленных соответствующими структурами, и оформить блок-схемы основных алгоритмов, прокомментировать код.
5.  Тестирование и отладка текущей реализации и повтор при необходимости шагов 1-4.
6.  Оформить отчет о выполненной работе, включающий:
    *   титульный лист;
    *   описание лабораторной, её темы и цели;
    *   постановку задачи и дополнительные теоретические сведения к ней;
    *   описание модели: краткое описание особенностей;
    *   при необходимости – описание реализации с блок-схемами алгоритмов или ссылками на них;
    *   исходные данные;
    *   результаты счёта и времена их получения; описание результатов тестирования и отладки;
    *   графики (по 2 семейства графиков для величины каждого вида);
    *   вопросы и ответы на них;
    *   заключение (выводы);
    *   список ссылок на использованные материалы и источники;
    *   технический стиль оформления отчёта предполагает: отсутствие прямых утверждений от первого лица, в том числе включающих местоимения первого лица; наличие ссылок на результаты коллег с указанием авторства; отсутствие предметно не подпадающих под тему отчёта повествований, включая личный чувственный опыт; использование строго определённых технических и математических терминов в минимально достаточном для изложения количестве; при аналитическом сравнении полагание в первую очередь на количественные признаки и метрические шкалы; для неметрических признаков должна быть строго задана модель шкалы; максимальное приближение результатов анализа к «объективным»; указание на используемые модели и методы решения задач, включая анализ; аккуратность и единообразие оформления; полноту и правильность, соответствие принятым стандартам оформления рисунков, графиков, формул, таблиц; минимизацию числа орфографических, пунктуационных и иных синтаксических ошибок, отсутствие формулировок без смысла и с нерелевантным смыслом, включая неоднозначные формулировки.

## Варианты заданий по пятой лабораторной работе:
1.  Реализовать модель сети Джордана с логарифмической функцией активации (гиперболический арксинус).
2.  Реализовать модель сети Элмана с логарифмической функцией активации (гиперболический арксинус).
3.  Реализовать модель сети Джордана-Элмана с логарифмической функцией активации (гиперболический арксинус).
4.  Реализовать модель рекуррентной сети с цепью нейросетевых моделей долгой кратковременной памяти с линейной функцией активации выходного сигнала на скрытом слое.
5.  Реализовать модель рекуррентной сети с цепью нейросетевых моделей долгой кратковременной памяти с недовыпрямленной линейной функцией активации (Leaky ReLU) выходного сигнала на скрытом слое.
6.  Реализовать модель рекуррентной сети с цепью нейросетевых моделей долгой кратковременной памяти с логарифмической функцией активации (гиперболический арксинус) выходного сигнала на скрытом слое.
7.  Реализовать модель рекуррентной сети с цепью нейросетевых моделей долгой кратковременной памяти с функцией активации гиперболического тангенса выходного сигнала на скрытом слое.
8.  Реализовать модель сети Джордана с экспоненциальной линейной функцией активации (ELU).
9.  Реализовать модель сети Элмана с экспоненциальной линейной функцией активации (ELU).
10. Реализовать модель сети Джордана-Элмана с экспоненциальной линейной функцией активации (ELU).
11. Реализовать модель сети Джордана с недовыпрямленной линейной функцией активации (Leaky ReLU).
12. Реализовать модель сети Элмана с недовыпрямленной линейной функцией активации (Leaky ReLU).
13. Реализовать модель сети Джордана-Элмана с недовыпрямленной линейной функцией активации (Leaky ReLU).
14. Реализовать модель рекуррентной сети с цепью нейросетевых моделей управляемых рекуррентных блоков с функцией активации гиперболического тангенса выходного сигнала на скрытом слое.
15. Реализовать модель рекуррентной сети с цепью нейросетевых моделей управляемых рекуррентных блоков с логарифмической функцией активации (гиперболический арксинус) выходного сигнала на скрытом слое.
16. Реализовать модель рекуррентной сети с цепью нейросетевых моделей управляемых рекуррентных блоков с недовыпрямленной линейной функцией активации (Leaky ReLU) выходного сигнала на скрытом слое.
