## 226. Сложность обучения нейронной сети с учителем в отличие от обучения одного нейрона, заключается:
1) в отсутствии обратных связей
2) в наличии весовых коэффициентов
3) ✅ в отсутствии эталонных значений для нейронов скрытых слоёв
4) в наличии линейной функции активации
5) в наличии дифференцируемой функции активации

## 227. Какая форма параллелизма преобладает при обработке входных данных в искусственных нейронных сетях:
1) крупногранулярный параллелизм
2) параллелизм задач
3) параллелизм независимых ветвей
4) ✅ естественный параллелизм
5) скалярный параллелизм

## 228. Весовой коэффициент, имеющий значение больше нуля, называется:
1) замедляющим
2) ускоряющим
3) ✅ возбуждающим
4) мёртвым
5) тормозящим

## 229. Весовой коэффициент, имеющий значение меньше нуля, называется:
1) живым
2) замедляющим
3) ✅ тормозящим
4) ускоряющим
5) возбуждающим

## 230. Встречные синаптические связи, которые соединяют два разных нейрона одного слоя называются:
1) прямыми
2) ✅ перекрёстными
3) обратными
4) тормозящими
5) возбуждающими

## 231. Какова область значений бинарной пороговой функции (с бинарными значениями):
1) {-1,1}
2) ✅ {0,1}
3) [0,1]
4) {-1,0,1}
5) [-1,1]

## 232. Какова область значений биполярной пороговой функции (с биполярными значениями):
1) [0,1]
2) {0,1}
3) ✅ {-1,1}
4) [-1,1]
5) {-1,0,1}

## 233. Какова область значений линейной функции:
1) {0,1}
2) (-1,1)
3) [-1,1]
4) (0,1)
5) ✅ R

## 234. Какова область значений логистической (сигмоидной) функции:
1) {-1,1}
2) [0,1]
3) [-1,1]
4) {0,1}
5) ✅ (0,1)

## 235. Какое минимальное количество скрытых слоёв должна иметь нейронная сеть для решения любой задачи, которую решает однонаправленная нерекуррентная линейная нейронная сеть:
- 0

## 236. Какое минимальное количество скрытых слоёв должна иметь однонаправленная нерекуррентная нейронная сеть с линейным синаптическим преобразованием и монотонной функцией активации для реализации любой логической функции:
- 1

## 237. Какое минимальное количество скрытых слоёв должна иметь однонаправленная нерекуррентная нейронная сеть с линейным синаптическим преобразованием и ограниченной непрерывной нелинейной функцией активации для аппроксимации любой ограниченной непрерывной функции:
- 1

## 238. Какое минимальное количество скрытых слоёв должна иметь однонаправленная нерекуррентная нейронная сеть с линейным синаптическим преобразованием и монотонной функцией активации для установления принадлежности точки пространства входных сигналов произвольному полиэдру:
- 2

## 239. Какое минимальное количество скрытых слоёв должна иметь однонаправленная нерекуррентная нейронная сеть с линейным синаптическим преобразованием и монотонной функцией активации для установления принадлежности точки пространства входных сигналов произвольному выпуклому полиэдру:
- 1

## 240. Многократное распространение ошибки применяется для обучения:
1) рекуррентных сетей
2) сетей с недифференцируемой функцией активации
3) однослойных сетей
4) ✅ многослойных сетей
5) сетей с пороговой функцией активации

## 241. Нормализованным по длине вектором называется вектор:
1) абсолютное значение компонентов которого равно единице
2) значение компонентов которого не превышает единицы
3) длина которого не превышает единицу
4) абсолютное значение компонентов которого не превышает единицы
5) ✅ длина которого равна единице

## 242. Матрица всех вторых частных производных функции, в которой производные в одинаковых строках и столбцах берутся по одному и тому же аргументу, а в разных – по разным, называется:
1) гамильтонианом
2) якобианом
3) весовой матрицей
4) ✅ гессианом
5) градиентом

## 243. Метод обратного распространения ошибки требует наличия:
1) линейной функции активации
2) монотонной функции активации
3) ✅ дифференцируемой функции активации
4) нелинейной функции активации
5) непрерывной функции активации

## 244. Набор векторов входных сигналов и соответствующих эталонов, на которых производится обучение, называется:
1) слоем
2) ✅ обучающей выборкой
3) весовым вектором
4) правилом обучения
5) тестовой выборкой

## 245. Алгоритм обратного распространения ошибки предполагает инициализацию весовых коэффициентов:
1) одинаковыми значениями
2) ✅ значениями равномерно распределённой случайной величины
3) значениями нормально распределённой случайной величины
4) иррациональными числами
5) нулевыми значениями

## 246. Набор векторов входных сигналов и соответствующих эталонов, на которых производится тестирование, называется:
1) весовым вектором
2) правилом обучения
3) слоем
4) обучающей выборкой
5) ✅ тестовой выборкой

## 247. Коэффициентом обучения называется:
1) среднее квадрата разности выходных значений и эталонов
2) элемент входного вектора
3) вектор значений первых частных производных функции ошибки по набору аргументов
4) элемент весового вектора
5) ✅ положительный скалярный множитель основного приращения весового коэффициента

## 248. Среднеквадратической ошибкой называется:
1) вектор значений первых частных производных функции ошибки по набору аргументов
2) положительный скалярный множитель основного приращения весового коэффициента
3) разность эталонов и выходных значений
4) ✅ среднее квадрата разности выходных значений и эталонов
5) элемент весового вектора

## 249. Вид обучения, при котором обязательно наличие эталонов, называется:
1) ✅ обучением с учителем
2) самообучением
3) обучением по правилу Хебба
4) обучением без учителя
5) обучением по методу проекций

## 250. Градиентом функции ошибки называется:
1) ✅ вектор значений первых частных производных функции ошибки по набору аргументов
2) среднее квадрата разности выходных значений и эталонов
3) элемент входного вектора
4) элемент весового вектора
5) положительный скалярный множитель основного приращения весового коэффициента
